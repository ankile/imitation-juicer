{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fig_base = Path(\".\").absolute().parent / \"reports\" / \"figures\"\n",
    "\n",
    "fig_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main results grouped bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "tasks = [\"Square table\", \"Round table\", \"Lamp\", \"Stool\", \"Chair\"]\n",
    "methods = [\"BC\", \"Collect-and-Infer\", \"Trajectory Augmentation\", \"Negative guidance\"]\n",
    "# Generating random data for the methods within each task\n",
    "data = np.random.randint(0, 100, size=(len(tasks), len(methods)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Set position of bar on X axis\n",
    "barWidth = 0.2\n",
    "r1 = np.arange(len(data))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "\n",
    "# Make the plot\n",
    "ax.bar(\n",
    "    r1,\n",
    "    data[:, 0],\n",
    "    width=barWidth,\n",
    "    edgecolor=\"grey\",\n",
    "    label=\"BC\",\n",
    ")\n",
    "ax.bar(\n",
    "    r2,\n",
    "    data[:, 1],\n",
    "    width=barWidth,\n",
    "    edgecolor=\"grey\",\n",
    "    label=\"Collect-and-Infer\",\n",
    ")\n",
    "ax.bar(\n",
    "    r3,\n",
    "    data[:, 2],\n",
    "    width=barWidth,\n",
    "    edgecolor=\"grey\",\n",
    "    label=\"Trajectory Augmentation\",\n",
    ")\n",
    "ax.bar(\n",
    "    r4,\n",
    "    data[:, 3],\n",
    "    width=barWidth,\n",
    "    edgecolor=\"grey\",\n",
    "    label=\"Negative guidance\",\n",
    ")\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "ax.set_xlabel(\"Tasks\", fontweight=\"bold\")\n",
    "ax.set_xticks([r + barWidth for r in range(len(data))])\n",
    "ax.set_xticklabels(tasks)\n",
    "ax.set_ylabel(\"Success rate (%)\")\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Adding the \"Placeholder\" label\n",
    "plt.text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    \"Placeholder\",\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"red\",\n",
    "    fontsize=48,\n",
    "    alpha=0.75,\n",
    ")\n",
    "\n",
    "\n",
    "# Create legend & Show graphic\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Success rate of the methods for each task\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_base / \"success_rate.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare pretraining vision encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "categories = {\n",
    "    \"RN18, Scratch\": 49,\n",
    "    \"RN18, ImageNet\": 59,\n",
    "    \"Category 3\": 30,\n",
    "}\n",
    "\n",
    "# Define the color palette with reduced alpha value\n",
    "colors = [\"#FF6666\", \"#FF3333\", \"#FF0000\"]\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create bar chart with custom colors\n",
    "ax.bar(categories.keys(), categories.values(), color=colors, alpha=0.75)\n",
    "\n",
    "# Set y-axis to display percentages\n",
    "ax.yaxis.set_major_formatter(\"{x:.0f}%\")\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Categories\")\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_title(\"Bar Chart with Percentages\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the `round_table` augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitatively analyze trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.visualization.render_mp4 import (\n",
    "    mp4_from_pickle_jupyter,\n",
    "    unpickle_data,\n",
    "    pickle_data,\n",
    ")\n",
    "from src.common.files import get_raw_paths\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "base_dir = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_raw_paths(\n",
    "    environment=\"sim\",\n",
    "    demo_source=\"augmentation\",\n",
    "    demo_outcome=\"success\",\n",
    "    task=\"round_table\",\n",
    "    randomness=\"low\",\n",
    ")\n",
    "\n",
    "random.shuffle(paths)\n",
    "\n",
    "len(paths), paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teleop_paths = get_raw_paths(\n",
    "    environment=\"sim\",\n",
    "    demo_source=\"teleop\",\n",
    "    demo_outcome=\"success\",\n",
    "    task=\"round_table\",\n",
    "    randomness=\"low\",\n",
    ")\n",
    "\n",
    "random.shuffle(teleop_paths)\n",
    "\n",
    "len(teleop_paths), teleop_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in tqdm(teleop_paths):\n",
    "    data = unpickle_data(path)\n",
    "    idxs = np.where(np.array(data[\"augment_states\"]) == 1)[0]\n",
    "    if len(idxs) != 3:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = defaultdict(list)\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    data = unpickle_data(path)\n",
    "    images[data.get(\"critical_state\")].append(data[\"observations\"][0][\"color_image2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, len(v)) for k, v in images.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Iterate over the keys and images in the images dictionary\n",
    "for i, (key, image_list) in enumerate(images.items()):\n",
    "    if key is None:\n",
    "        continue\n",
    "\n",
    "    # Create a 3-by-3 grid of subplots with no space between axes\n",
    "    img_list = image_list[:9]\n",
    "\n",
    "    h, w = img_list[0].shape[:2]\n",
    "\n",
    "    # Center crop each image to be square\n",
    "    img_list = [img[:, (w - h) // 2 : (w + h) // 2] for img in img_list]\n",
    "\n",
    "    # Remove 10 pixels from each edge of each image\n",
    "    img_list = [img[10:-10, 10:-10] for img in img_list]\n",
    "\n",
    "    # Concatenate the images into a single 3-by-3 image\n",
    "    img = [np.concatenate(img_list[i : i + 3], axis=1) for i in range(0, 9, 3)]\n",
    "    img = np.concatenate(img, axis=0)\n",
    "\n",
    "    # Create a new figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Remove the x and y ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Remove everything\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Save without a white edge around the image\n",
    "    plt.savefig(\n",
    "        fig_base / f\"augmentation_grid_{i}.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot coverage of new trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from src.common.files import get_processed_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_path, teleop_path = sorted(\n",
    "    get_processed_paths(\n",
    "        environment=\"sim\",\n",
    "        demo_source=[\"teleop\", \"augmentation\"],\n",
    "        demo_outcome=\"success\",\n",
    "        task=\"round_table\",\n",
    "        randomness=\"low\",\n",
    "    )\n",
    ")\n",
    "\n",
    "aug_path, teleop_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_aug = zarr.open(str(aug_path), mode=\"r\")\n",
    "z_teleop = zarr.open(str(teleop_path), mode=\"r\")\n",
    "\n",
    "ends_aug = z_aug[\"episode_ends\"][:]\n",
    "ends_teleop = z_teleop[\"episode_ends\"][:]\n",
    "\n",
    "pos_teleop = z_teleop[\"robot_state\"][:, :3]\n",
    "pos_aug = z_aug[\"robot_state\"][:, :3]\n",
    "\n",
    "\n",
    "# Split the data into episodes\n",
    "pos_teleop = np.split(pos_teleop, ends_teleop[:-1])\n",
    "pos_aug = np.split(pos_aug, ends_aug[:-1])\n",
    "\n",
    "# Filter out only episodes with critical state index (i.e., not -1)\n",
    "cs_idx = z_aug[\"critical_state_id\"][:]\n",
    "aug_for_state = defaultdict(list)\n",
    "for i, (pos, idx) in enumerate(zip(pos_aug, cs_idx)):\n",
    "    if idx != -1:\n",
    "        aug_for_state[idx].append(pos)\n",
    "\n",
    "cs_insert_leg = aug_for_state[0]\n",
    "cs_grasp_base = aug_for_state[1]\n",
    "cs_insert_base = aug_for_state[2]\n",
    "\n",
    "# Count number in each state\n",
    "len(cs_insert_leg), len(cs_grasp_base), len(cs_insert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_teleop.critical_state_idxs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat them together again\n",
    "# pos_teleop = np.concatenate(pos_teleop)\n",
    "# pos_aug = np.concatenate(pos_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ends_teleop), len(pos_teleop), len(ends_aug), len(pos_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the state-space coverage in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add teleop scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=pos_teleop.T[0],\n",
    "        y=pos_teleop.T[1],\n",
    "        z=pos_teleop.T[2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            opacity=0.3,\n",
    "            # color=\"#BCD3FF\",\n",
    "        ),\n",
    "        name=f\"Teleop (n={len(ends_teleop)})\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add augmentation scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=pos_aug.T[0],\n",
    "        y=pos_aug.T[1],\n",
    "        z=pos_aug.T[2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            opacity=0.5,\n",
    "            # color=\"#FFB8B8\",\n",
    "        ),\n",
    "        name=f\"Augmentation (n={len(ends_aug)})\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update the layout to make it look nice and square\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"y\",\n",
    "        zaxis_title=\"z\",\n",
    "        aspectmode=\"cube\",  # Ensures equal aspect ratio for a square look\n",
    "        camera=dict(\n",
    "            up=dict(x=0, y=0, z=0.7),  # Sets the z-axis to be up\n",
    "            center=dict(x=0, y=0, z=0),  # Centers the view\n",
    "            eye=dict(x=0.9, y=0.9, z=0.9),  # Adjust these values to zoom in\n",
    "        ),\n",
    "    ),\n",
    "    legend=dict(yanchor=\"top\", y=0.9, xanchor=\"left\", x=0.01, font=dict(size=24)),\n",
    "    margin=dict(l=0, r=0, b=0, t=0),  # Reduce default margins\n",
    "    width=800,  # Adjust figure width\n",
    "    height=800,  # Adjust figure height to make it square\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(zaxis=dict(range=[0, 0.4]))  # Focuses the z-axis to show only 0 to 50\n",
    ")\n",
    "\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# To save the figure\n",
    "fig.write_image(str(fig_base / \"teleop_augmentation.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a big grid of augmented trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs2 = z_aug[\"color_image2\"][:]\n",
    "\n",
    "imgs2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it into episodes\n",
    "imgs2 = np.split(imgs2, ends_aug[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs2), imgs2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vids = 15\n",
    "\n",
    "vids = []\n",
    "for i in range(0, len(imgs2), n_vids):\n",
    "    imgs = imgs2[i : i + n_vids]\n",
    "\n",
    "    vid = np.concatenate(imgs, axis=0)\n",
    "\n",
    "    vids.append(vid)\n",
    "\n",
    "len(vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_height, crop_width = 224, 224\n",
    "nrows, ncols = 5, 9\n",
    "h, w = vids[0].shape[1:3]\n",
    "\n",
    "h, w, crop_height, crop_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h - crop_height) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = vids[: nrows * ncols]\n",
    "\n",
    "# Cut the videos to the same length\n",
    "min_len = min(len(vid) for vid in vids)\n",
    "\n",
    "# Cut and center crop the videos\n",
    "vids = [\n",
    "    vid[\n",
    "        :min_len,\n",
    "        (h - crop_height) // 2 : (h + crop_height) // 2,\n",
    "        (w - crop_width) // 2 : (w + crop_width) // 2,\n",
    "    ]\n",
    "    for vid in vids\n",
    "]\n",
    "\n",
    "len(vids), vids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the videos into a single 4-by-4 video\n",
    "vid = [\n",
    "    np.concatenate(vids[i : i + ncols], axis=1) for i in range(0, nrows * ncols, ncols)\n",
    "]\n",
    "vid = np.concatenate(vid, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.render_mp4 import create_mp4_jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mp4_jupyter(vid, \"augmentation_grid.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make videos of rollouts for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_teleop = sorted(\n",
    "    get_processed_paths(\n",
    "        environment=\"sim\",\n",
    "        demo_source=[\"teleop\"],\n",
    "        demo_outcome=\"success\",\n",
    "        task=[\"round_table\", \"lamp\", \"square_table\", \"one_leg\"],\n",
    "        randomness=\"low\",\n",
    "    )\n",
    ")\n",
    "\n",
    "z_teleop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render 3 videoes from each task\n",
    "\n",
    "for path in z_teleop:\n",
    "    z = zarr.open(str(path), mode=\"r\")\n",
    "    imgs = z[\"color_image2\"][: z[\"episode_ends\"][2]]\n",
    "\n",
    "    # Split it into episodes\n",
    "    imgs = np.split(imgs, z[\"episode_ends\"][:2])\n",
    "\n",
    "    for i, img in enumerate(imgs[:3]):\n",
    "        create_mp4_jupyter(img, f\"teleop_{path.parts[-5:-1]}_{i}.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a plot for the Bootstrap experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "x = range(1, 5)  # For example, 4 groups\n",
    "category_a = [5, 10, 15, 20]\n",
    "category_b = [3, 7, 12, 17]\n",
    "category_c = [1, 4, 9, 14]\n",
    "line_data = [10, 15, 20, 25]\n",
    "\n",
    "# Plotting the stacked bars\n",
    "plt.bar(x, category_a, label=\"Category A\", color=\"lightblue\")\n",
    "plt.bar(x, category_b, bottom=category_a, label=\"Category B\", color=\"lightgreen\")\n",
    "# To stack category_c, we need to add the values of category_a and category_b for the bottom parameter\n",
    "combined_bottom = [a + b for a, b in zip(category_a, category_b)]\n",
    "plt.bar(x, category_c, bottom=combined_bottom, label=\"Category C\", color=\"lightcoral\")\n",
    "\n",
    "# Adding the line plot\n",
    "plt.plot(x, line_data, label=\"Line Data\", color=\"black\", marker=\"o\", linestyle=\"--\")\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel(\"X axis\")\n",
    "plt.ylabel(\"Y axis\")\n",
    "plt.title(\"Stacked Bar Chart with Line\")\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "x = range(1, 4)\n",
    "category_a = [10, 10, 10]\n",
    "category_b = [0, 40, 90]\n",
    "line_data = [19, 55, 71]\n",
    "\n",
    "x_labels = [\"Iteration 1\", \"Iteration 2\", \"Iteration 3\"]\n",
    "# x = range(1, 3)\n",
    "# category_a = [10, 10]\n",
    "# category_b = [0, 90]\n",
    "# line_data = [19, 71]\n",
    "\n",
    "# x_labels = [\"Iteration 1\", \"Iteration 2\"]\n",
    "\n",
    "# Creating a subplot\n",
    "fig, ax1 = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# Plotting the stacked bars on the first axis\n",
    "bar1 = ax1.bar(x, category_a, label=\"Manual demos\", color=\"#BCD3FF\")\n",
    "bar2 = ax1.bar(x, category_b, bottom=category_a, label=\"Synthetic\", color=\"#FFB8B8\")\n",
    "ax1.set_xlabel(\"# Iteration of JUICER\")\n",
    "ax1.set_ylabel(\"Number of Demos\")\n",
    "\n",
    "# Set custom x-axis tick labels\n",
    "ax1.set_xticks(x)  # Set the positions of the x-ticks\n",
    "ax1.set_xticklabels(x_labels)  # Set the custom labels for each tick\n",
    "\n",
    "# Create a second y-axis for the line plot\n",
    "ax2 = ax1.twinx()\n",
    "(line,) = ax2.plot(\n",
    "    x, line_data, label=\"Line Data\", color=\"black\", marker=\"o\", linestyle=\"--\"\n",
    ")\n",
    "ax2.set_ylabel(\"Average Success Rate (%)\")\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "# Format the right y-axis as percent\n",
    "ax2.yaxis.set_major_formatter(PercentFormatter(decimals=0))\n",
    "\n",
    "\n",
    "# Adding labels above each point on the line\n",
    "for i, txt in enumerate(line_data):\n",
    "    ax2.text(x[i], line_data[i] + 2, f\"{txt}%\", ha=\"center\", va=\"bottom\", color=\"black\")\n",
    "\n",
    "\n",
    "# Creating combined legend\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "handles.append(line)  # Adding the line plot handle to the list of handles\n",
    "labels.append(\"Line Data\")  # Adding the line plot label to the list of labels\n",
    "\n",
    "# Displaying the combined legend\n",
    "ax1.legend(handles, labels, loc=\"upper left\")\n",
    "\n",
    "# fig.suptitle(\"Stacked Bar Chart with Line\")\n",
    "\n",
    "plt.savefig(fig_base / \"bootstrap_success.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(1, 4)\n",
    "category_a = [10, 50, 10]\n",
    "category_b = [0, 0, 90]\n",
    "line_data = [19, 55, 71]\n",
    "\n",
    "x_labels = [\"10 demos\", \"50 demos\", \"10 demos\\n+JUICER\"]\n",
    "\n",
    "# Adjusting category_a and category_b to fit the total heights specified by line_data\n",
    "# Calculate the total of category_a and category_b for each stack\n",
    "totals = [a + b for a, b in zip(category_a, category_b)]\n",
    "\n",
    "# Calculate the adjusted heights\n",
    "adjusted_a = [a / t * l if t else 0 for a, t, l in zip(category_a, totals, line_data)]\n",
    "adjusted_b = [l - a for a, l in zip(adjusted_a, line_data)]\n",
    "\n",
    "# Creating a subplot\n",
    "fig, ax1 = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# Plotting the adjusted stacked bars on the first axis\n",
    "bar1 = ax1.bar(x, adjusted_a, label=\"Manual demos\", color=\"#BCD3FF\")\n",
    "bar2 = ax1.bar(x, adjusted_b, bottom=adjusted_a, label=\"Synthetic\", color=\"#FFB8B8\")\n",
    "\n",
    "# ax1.set_xlabel(\"# Iteration of JUICER\")\n",
    "ax1.set_ylabel(\"Average Success Rate (%)\")\n",
    "\n",
    "# Set custom x-axis tick labels\n",
    "ax1.set_xticks(x)  # Set the positions of the x-ticks\n",
    "ax1.set_xticklabels(x_labels)  # Set the custom labels for each tick\n",
    "\n",
    "# Adjust y-axis labels to reflect the actual values\n",
    "ax1.set_ylim(\n",
    "    0, max(line_data) + 10\n",
    ")  # Adjust ylim to fit the highest line_data value plus some margin\n",
    "\n",
    "# Add data labels atop each bar\n",
    "for xpos, value in zip(x, line_data):\n",
    "    ax1.text(xpos, value, f\"{value}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Creating legend\n",
    "ax1.legend(loc=\"upper left\", frameon=False)\n",
    "\n",
    "\n",
    "plt.savefig(fig_base / \"bootstrap_success.pdf\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render new rollouts in high resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.visualization.render_mp4 import (\n",
    "    create_mp4_jupyter,\n",
    "    unpickle_data,\n",
    ")\n",
    "import cv2\n",
    "from src.common.files import get_raw_paths\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "base_dir = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rollout_video(demo_outcome=\"success\", task_subset=None, n_videos=1):\n",
    "    for task in tqdm([\"one_leg\", \"round_table\", \"lamp\", \"square_table\"]):\n",
    "\n",
    "        if task_subset is not None and task not in task_subset:\n",
    "            continue\n",
    "\n",
    "        paths = get_raw_paths(\n",
    "            environment=\"sim\",\n",
    "            demo_source=\"rollout\",\n",
    "            demo_outcome=demo_outcome,\n",
    "            task=task,\n",
    "            randomness=\"low\",\n",
    "        )\n",
    "\n",
    "        fps = 20\n",
    "\n",
    "        paths = sorted(paths, reverse=True)[:n_videos]\n",
    "\n",
    "        for i, path in enumerate(paths):\n",
    "            print(f\"Rendering {path}\")\n",
    "            imgs = []\n",
    "            for obs in tqdm(unpickle_data(path)[\"observations\"]):\n",
    "\n",
    "                img = obs[\"color_image2\"]\n",
    "\n",
    "                # Add the \"2x\" text overlay in the lower-right corner\n",
    "                img_with_text = cv2.putText(\n",
    "                    img,\n",
    "                    f\"{fps//10}x\",\n",
    "                    (\n",
    "                        img.shape[1] - 100,\n",
    "                        img.shape[0] - 40,\n",
    "                    ),  # Adjusted position of the text\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "                    2,  # Font scale (doubled)\n",
    "                    (255, 255, 255),  # Text color (white)\n",
    "                    4,  # Text thickness (doubled)\n",
    "                    cv2.LINE_AA,  # Line type for better rendering\n",
    "                )\n",
    "\n",
    "                imgs.append(img_with_text)\n",
    "\n",
    "            imgs = np.array(imgs)\n",
    "            create_mp4_jupyter(\n",
    "                imgs, base_dir / f\"rollout_{task}_{demo_outcome}_{i}.mp4\", fps=fps\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following paths:\n",
      "    /data/scratch-oc40/pulkitag/ankile/furniture-data/raw/sim/square_table/rollout/low/success/*.pkl*\n",
      "Rendering /data/scratch-oc40/pulkitag/ankile/furniture-data/raw/sim/square_table/rollout/low/success/2024-03-28T16:52:10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2239/2239 [00:00<00:00, 27641.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/rollout_square_table_success_0.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls src=\"tmp/rollout_square_table_success_0.mp4\" width=\"640\" height=\"480\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering /data/scratch-oc40/pulkitag/ankile/furniture-data/raw/sim/square_table/rollout/low/success/2024-03-19T07:59:07.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2379/2379 [00:00<00:00, 27420.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/rollout_square_table_success_1.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls src=\"tmp/rollout_square_table_success_1.mp4\" width=\"640\" height=\"480\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [06:21<00:00, 95.32s/it]\n"
     ]
    }
   ],
   "source": [
    "render_rollout_video(demo_outcome=\"success\", task_subset=[\"square_table\"], n_videos=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following paths:\n",
      "    /data/scratch-oc40/pulkitag/ankile/furniture-data/raw/sim/round_table/rollout/low/failure/*.pkl*\n",
      "Rendering /data/scratch-oc40/pulkitag/ankile/furniture-data/raw/sim/round_table/rollout/low/failure/2024-03-28T17:15:19.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 26007.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as tmp/rollout_round_table_failure_0.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls src=\"tmp/rollout_round_table_failure_0.mp4\" width=\"640\" height=\"480\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering /data/scratch-oc40/pulkitag/ankile/furniture-data/raw/sim/round_table/rollout/low/failure/2024-03-28T17:12:59.pkl\n"
     ]
    }
   ],
   "source": [
    "render_rollout_video(demo_outcome=\"failure\", task_subset=[\"round_table\"], n_videos=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerender old rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_38' (/data/scratch/ankile/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /data/scratch/ankile/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
      "PyTorch version 2.2.1+cu121\n",
      "Device count 1\n",
      "/data/scratch/ankile/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /data/scratch/ankile/.cache as PyTorch extensions root...\n",
      "Emitting ninja build file /data/scratch/ankile/.cache/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gymtorch...\n"
     ]
    }
   ],
   "source": [
    "from src.gym import get_env\n",
    "\n",
    "from pathlib import Path\n",
    "from src.visualization.render_mp4 import (\n",
    "    mp4_from_pickle_jupyter,\n",
    "    create_mp4_jupyter,\n",
    "    unpickle_data,\n",
    "    pickle_data,\n",
    ")\n",
    "from src.common.files import get_raw_paths\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from isaacgym import gymtorch, gymapi\n",
    "import torch\n",
    "\n",
    "\n",
    "base_dir = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to PVD\n",
      "+++ Using GPU PhysX\n",
      "Physics Engine: PhysX\n",
      "Physics Device: cuda:0\n",
      "GPU Pipeline: enabled\n",
      "*** Failed to load '/data/scratch/ankile/furniture-diffusion/assets/furniture/urdf/base_tag.urdf'*** Failed to load '/data/scratch/ankile/furniture-diffusion/assets/furniture/urdf/background.urdf'*** Failed to load '/data/scratch/ankile/furniture-diffusion/assets/furniture/urdf/table.urdf'*** Failed to load '/data/scratch/ankile/furniture-diffusion/assets/furniture/urdf/obstacle_front.urdf'*** Failed to load '/data/scratch/ankile/furniture-diffusion/assets/furniture/urdf/obstacle_side.urdf'*** Failed to load '/data/scratch/ankile/furniture-diffusion/assets/franka_description_ros/franka_description/robots/franka_panda.urdf'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Error] [carb.gym.plugin] Failed to parse URDF file 'furniture/urdf/base_tag.urdf'\n",
      "[Error] [carb.gym.plugin] Failed to import URDF file 'furniture/urdf/base_tag.urdf'\n",
      "[Error] [carb.gym.plugin] *** Failed to load 'furniture/urdf/base_tag.urdf' from '/data/scratch/ankile/furniture-diffusion/assets'\n",
      "[Error] [carb.gym.plugin] Failed to parse URDF file 'furniture/urdf/background.urdf'\n",
      "[Error] [carb.gym.plugin] Failed to import URDF file 'furniture/urdf/background.urdf'\n",
      "[Error] [carb.gym.plugin] *** Failed to load 'furniture/urdf/background.urdf' from '/data/scratch/ankile/furniture-diffusion/assets'\n",
      "[Error] [carb.gym.plugin] Failed to parse URDF file 'furniture/urdf/table.urdf'\n",
      "[Error] [carb.gym.plugin] Failed to import URDF file 'furniture/urdf/table.urdf'\n",
      "[Error] [carb.gym.plugin] *** Failed to load 'furniture/urdf/table.urdf' from '/data/scratch/ankile/furniture-diffusion/assets'\n",
      "[Error] [carb.gym.plugin] Failed to parse URDF file 'furniture/urdf/obstacle_front.urdf'\n",
      "[Error] [carb.gym.plugin] Failed to import URDF file 'furniture/urdf/obstacle_front.urdf'\n",
      "[Error] [carb.gym.plugin] *** Failed to load 'furniture/urdf/obstacle_front.urdf' from '/data/scratch/ankile/furniture-diffusion/assets'\n",
      "[Error] [carb.gym.plugin] Failed to parse URDF file 'furniture/urdf/obstacle_side.urdf'\n",
      "[Error] [carb.gym.plugin] Failed to import URDF file 'furniture/urdf/obstacle_side.urdf'\n",
      "[Error] [carb.gym.plugin] *** Failed to load 'furniture/urdf/obstacle_side.urdf' from '/data/scratch/ankile/furniture-diffusion/assets'\n",
      "[Error] [carb.gym.plugin] Failed to parse URDF file 'franka_description_ros/franka_description/robots/franka_panda.urdf'\n",
      "[Error] [carb.gym.plugin] Failed to import URDF file 'franka_description_ros/franka_description/robots/franka_panda.urdf'\n",
      "[Error] [carb.gym.plugin] *** Failed to load 'franka_description_ros/franka_description/robots/franka_panda.urdf' from '/data/scratch/ankile/furniture-diffusion/assets'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'k_ee_link'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mget_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfurniture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchair\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresize_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapril_tags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/scratch/ankile/furniture-diffusion/src/gym/__init__.py:32\u001b[0m, in \u001b[0;36mget_env\u001b[0;34m(gpu_id, furniture, num_envs, randomness, max_env_steps, resize_img, act_rot_repr, ctrl_mode, action_type, april_tags, verbose, headless)\u001b[0m\n\u001b[1;32m     28\u001b[0m     furniture_sim_env\u001b[38;5;241m.\u001b[39mASSET_ROOT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m     29\u001b[0m         Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mabsolute() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress_all_output(\u001b[38;5;129;01mnot\u001b[39;00m verbose):\n\u001b[0;32m---> 32\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFurnitureSim-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfurniture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfurniture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specifies the type of furniture [lamp | square_table | desk | drawer | cabinet | round_table | stool | chair | one_leg].\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of parallel environments.\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresize_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, images are resized to 224 x 224.\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_robot_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, robot state is concatenated to the observation.\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheadless\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, simulation runs without GUI.\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Includes the parts poses in the observation for resetting\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobs_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_VISUAL_OBS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparts_poses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_device_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraphics_device_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_assembled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, the environment is initialized with assembled furniture.\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp_step_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, env.step() returns Numpy arrays.\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchannel_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, images are returned in channel first format.\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Level of randomness in the environment [low | med | high].\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhigh_random_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Index of the high randomness level (range: [0-2]). Default -1 will randomly select the index within the range.\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_camera_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, the initial camera inputs are saved.\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, videos of the wrist and front cameras' RGB inputs are recorded.\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_env_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Maximum number of steps per episode.\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact_rot_repr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_rot_repr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Representation of rotation for action space. Options are 'quat' and 'axis'.\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctrl_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctrl_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Control mode for the robot. Options are 'osc' and 'diffik'.\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Action type for the robot. Options are 'delta' and 'pos'.\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If true, prints debug information.\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m/data/scratch/ankile/miniconda3/envs/ilgpu/lib/python3.8/site-packages/gym/envs/registration.py:235\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/scratch/ankile/miniconda3/envs/ilgpu/lib/python3.8/site-packages/gym/envs/registration.py:129\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking new env: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[1;32m    128\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec(path)\n\u001b[0;32m--> 129\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m/data/scratch/ankile/miniconda3/envs/ilgpu/lib/python3.8/site-packages/gym/envs/registration.py:90\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[0;32m---> 90\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m spec \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/data/scratch/ankile/furniture-bench/furniture_bench/envs/furniture_sim_env.py:166\u001b[0m, in \u001b[0;36mFurnitureSimEnv.__init__\u001b[0;34m(self, furniture, num_envs, resize_img, obs_keys, concat_robot_state, manual_label, manual_done, headless, compute_device_id, graphics_device_id, init_assembled, np_step_out, channel_first, randomness, high_random_idx, save_camera_input, record, max_env_steps, act_rot_repr, action_type, ctrl_mode, ee_laser, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_lights()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimport_assets()\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_envs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_viewer()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_camera()\n",
      "File \u001b[0;32m/data/scratch/ankile/furniture-bench/furniture_bench/envs/furniture_sim_env.py:246\u001b[0m, in \u001b[0;36mFurnitureSimEnv.create_envs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_tag_from_robot_mat \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrobot\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag_base_from_robot_base\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    245\u001b[0m franka_link_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misaac_gym\u001b[38;5;241m.\u001b[39mget_asset_rigid_body_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfranka_asset)\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfranka_ee_index \u001b[38;5;241m=\u001b[39m \u001b[43mfranka_link_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk_ee_link\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfranka_base_index \u001b[38;5;241m=\u001b[39m franka_link_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpanda_link0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Parts assets.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Create assets.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'k_ee_link'"
     ]
    }
   ],
   "source": [
    "env = get_env(\n",
    "    gpu_id=0,\n",
    "    furniture=\"chair\",\n",
    "    num_envs=1,\n",
    "    randomness=\"low\",\n",
    "    resize_img=False,\n",
    "    april_tags=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_raw_paths(\n",
    "    environment=\"sim\",\n",
    "    demo_source=\"teleop\",\n",
    "    demo_outcome=\"success\",\n",
    "    task=\"one_leg\",\n",
    "    randomness=\"low\",\n",
    ")\n",
    "\n",
    "path = sorted(paths, reverse=True)[0]\n",
    "\n",
    "print(f\"Rendering {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = unpickle_data(path)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs = env.reset()\n",
    "imgs = []\n",
    "\n",
    "root_positions = env.root_tensor[:, 0:3]\n",
    "root_orientations = env.root_tensor[:, 3:7]\n",
    "root_linvels = env.root_tensor[:, 7:10]\n",
    "root_angvels = env.root_tensor[:, 10:13]\n",
    "\n",
    "dof_pos = env.dof_pos\n",
    "dof_vel = env.dof_vel\n",
    "\n",
    "rb_states = env.rb_states\n",
    "\n",
    "num_actors = env.root_tensor.shape[0]\n",
    "\n",
    "for obs in tqdm(data[\"observations\"]):\n",
    "    imgs.append(new_obs[\"color_image2\"].squeeze().cpu().numpy())\n",
    "\n",
    "    # Set the root state directly\n",
    "    offsets = (\n",
    "        torch.tensor([0, 0.0, 0.1]).repeat(num_actors).reshape(-1, 3).float().cuda()\n",
    "    )\n",
    "    root_positions += offsets\n",
    "    env.isaac_gym.set_actor_root_state_tensor(\n",
    "        env.sim, gymtorch.unwrap_tensor(env.root_tensor)\n",
    "    )\n",
    "\n",
    "    # Set the DOF state directly\n",
    "    # dof_pos += 0.1\n",
    "    # env.isaac_gym.set_dof_state_tensor(env.sim, gymtorch.unwrap_tensor(env.dof_states))\n",
    "\n",
    "    # Set the rigid body state directly\n",
    "    # rb_states[:, 0:3] += 0.1\n",
    "    # env.isaac_gym.set_rigid_body_state_tensor(env.sim, gymtorch.unwrap_tensor(rb_states))\n",
    "\n",
    "    # Update all the renderers\n",
    "    env.isaac_gym.fetch_results(env.sim, True)\n",
    "    env.isaac_gym.step_graphics(env.sim)\n",
    "\n",
    "    # # Refresh tensors\n",
    "    env.isaac_gym.refresh_dof_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_dof_force_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_rigid_body_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_jacobian_tensors(env.sim)\n",
    "    env.isaac_gym.refresh_mass_matrix_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.render_all_camera_sensors(env.sim)\n",
    "    env.isaac_gym.start_access_image_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.end_access_image_tensors(env.sim)\n",
    "\n",
    "    # Get the image\n",
    "    new_obs = env.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the video\n",
    "imgs = np.array(imgs)\n",
    "create_mp4_jupyter(imgs, base_dir / f\"teleop_highres_one_leg_2.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.isaac_gym.get_sim_dof_count(env.sim), env.isaac_gym.get_sim_rigid_body_count(\n",
    "    env.sim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.isaac_gym.fetch_results(self.sim, True)\n",
    "# self.isaac_gym.step_graphics(self.sim)\n",
    "\n",
    "# # Refresh tensors.\n",
    "# self.isaac_gym.refresh_dof_state_tensor(self.sim)\n",
    "# self.isaac_gym.refresh_dof_force_tensor(self.sim)\n",
    "# self.isaac_gym.refresh_rigid_body_state_tensor(self.sim)\n",
    "# self.isaac_gym.refresh_jacobian_tensors(self.sim)\n",
    "# self.isaac_gym.refresh_mass_matrix_tensors(self.sim)\n",
    "# self.isaac_gym.render_all_camera_sensors(self.sim)\n",
    "\n",
    "# self.isaac_gym.start_access_image_tensors(self.sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_robot_state(observations):\n",
    "    robot_states = [obs[\"robot_state\"] for obs in observations]\n",
    "    parts_poses = [obs[\"parts_poses\"] for obs in observations]\n",
    "\n",
    "    # Remove the first robot_state entry\n",
    "    robot_states = robot_states[3:]\n",
    "\n",
    "    # Align the robot_states with parts_poses\n",
    "    aligned_observations = []\n",
    "    for i in range(len(robot_states)):\n",
    "        aligned_obs = {\n",
    "            \"robot_state\": robot_states[i],\n",
    "            \"color_image1\": observations[i][\"color_image1\"],\n",
    "            \"color_image2\": observations[i][\"color_image2\"],\n",
    "            \"parts_poses\": parts_poses[i],\n",
    "        }\n",
    "        aligned_observations.append(aligned_obs)\n",
    "\n",
    "    return aligned_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "imgs = []\n",
    "\n",
    "observations = data[\"observations\"]\n",
    "observations = offset_robot_state(observations)\n",
    "\n",
    "for obs in tqdm(data[\"observations\"]):\n",
    "    # Set the desired state\n",
    "    env.reset_env_to(env_idx=0, state=obs)\n",
    "\n",
    "    # torque_action = torch.zeros_like(env.dof_pos)\n",
    "    # env.isaac_gym.set_dof_actuation_force_tensor(\n",
    "    #     env.sim, gymtorch.unwrap_tensor(torque_action)\n",
    "    # )\n",
    "\n",
    "    # num_dofs = env.isaac_gym.get_sim_dof_count(env.sim)\n",
    "    # actions = torch.zeros(num_dofs).float().cuda()\n",
    "    # env.isaac_gym.set_dof_actuation_force_tensor(env.sim, gymtorch.unwrap_tensor(actions))\n",
    "\n",
    "    env.isaac_gym.simulate(env.sim)\n",
    "\n",
    "    # env.reset_env_to(env_idx=0, state=obs)\n",
    "\n",
    "    # Update all the renderers\n",
    "    env.isaac_gym.fetch_results(env.sim, True)\n",
    "    env.isaac_gym.step_graphics(env.sim)\n",
    "\n",
    "    # Refresh tensors\n",
    "    env.isaac_gym.refresh_dof_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_dof_force_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_rigid_body_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_jacobian_tensors(env.sim)\n",
    "    env.isaac_gym.refresh_mass_matrix_tensors(env.sim)\n",
    "\n",
    "    # env.isaac_gym.refresh_actor_root_state_tensor(env.sim)\n",
    "\n",
    "    env.isaac_gym.render_all_camera_sensors(env.sim)\n",
    "    env.isaac_gym.start_access_image_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.end_access_image_tensors(env.sim)\n",
    "\n",
    "    # Get the image\n",
    "    obs = env.get_observation()\n",
    "\n",
    "    imgs.append(obs[\"color_image2\"].squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "imgs = []\n",
    "\n",
    "for obs in tqdm(data[\"observations\"]):\n",
    "    # env.reset_env_to(env_idx=0, state=obs)\n",
    "\n",
    "    # Set the desired state for robot joint positions\n",
    "    dof_pos = (\n",
    "        torch.from_numpy(\n",
    "            np.concatenate(\n",
    "                [\n",
    "                    obs[\"robot_state\"][\"joint_positions\"],\n",
    "                    np.array([obs[\"robot_state\"][\"gripper_width\"] / 2] * 2),\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        .float()\n",
    "        .cuda()\n",
    "    )  # Convert the tensor to float32\n",
    "    # env.reset_franka_all(dof_pos)\n",
    "    dof_vel = torch.zeros_like(dof_pos).cuda()  # Set DOF velocities to zero\n",
    "\n",
    "    # Concatenate dof_pos and dof_vel along the second dimension\n",
    "    dof_state = torch.stack((dof_pos, dof_vel), dim=1)\n",
    "\n",
    "    # Get the actor index correctly\n",
    "    actor_idx = env.franka_actor_idxs_all_t[0].reshape(1, 1).cuda()\n",
    "\n",
    "    env.isaac_gym.set_dof_state_tensor_indexed(\n",
    "        env.sim, gymtorch.unwrap_tensor(dof_state), gymtorch.unwrap_tensor(actor_idx), 1\n",
    "    )\n",
    "\n",
    "    # Set the desired state for parts poses\n",
    "    parts_poses = (\n",
    "        torch.from_numpy(obs[\"parts_poses\"]).float().cuda()\n",
    "    )  # Convert the tensor to float32\n",
    "    env.isaac_gym.set_actor_root_state_tensor(\n",
    "        env.sim, gymtorch.unwrap_tensor(parts_poses)\n",
    "    )\n",
    "\n",
    "    # Update all the renderers\n",
    "    env.isaac_gym.fetch_results(env.sim, True)\n",
    "    env.isaac_gym.step_graphics(env.sim)\n",
    "\n",
    "    # Refresh tensors\n",
    "    env.isaac_gym.refresh_dof_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_dof_force_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_rigid_body_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_jacobian_tensors(env.sim)\n",
    "    env.isaac_gym.refresh_mass_matrix_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.render_all_camera_sensors(env.sim)\n",
    "    env.isaac_gym.start_access_image_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.end_access_image_tensors(env.sim)\n",
    "\n",
    "    # Get the image\n",
    "    obs = env.get_observation()\n",
    "\n",
    "    imgs.append(obs[\"color_image2\"].squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main results bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Increase font size globally and set font to Times New Roman\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.size\": 12,  # Adjust the base font size as needed\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        \"legend.fontsize\": \"large\",  # Adjust legend font size\n",
    "        \"axes.labelsize\": \"large\",  # Adjust axis labels font size\n",
    "        \"axes.titlesize\": \"x-large\",  # Adjust axis title font size\n",
    "        \"xtick.labelsize\": \"large\",  # Adjust X-axis tick label size\n",
    "        \"ytick.labelsize\": \"large\",  # Adjust Y-axis tick label size\n",
    "    }\n",
    ")\n",
    "\n",
    "# Original data from the LaTeX table\n",
    "data = {\n",
    "    \"Method\": [\n",
    "        \"MLP-NC\",\n",
    "        \"MLP-C\",\n",
    "        \"DP-BC\",\n",
    "        \"State noise\",\n",
    "        \"Traj. Aug.\",\n",
    "        \"Col.-Inf.\",\n",
    "        \"TA & CI\",\n",
    "        \"Multi-task\",\n",
    "    ],\n",
    "    \"One leg Avg\": [0, 40, 59, 66, 66, 75, 76, 59],\n",
    "    \"One leg Max\": [0, 52, 68, 70, 73, 79, 83, 63],\n",
    "    \"Round table Avg\": [0, 9, 18, 9, 28, 24, 32, 25],\n",
    "    \"Round table Max\": [0, 15, 21, 10, 33, 27, 35, 35],\n",
    "    \"Lamp Avg\": [None, 3, 6, 6, 9, 18, 29, 14],\n",
    "    \"Lamp Max\": [None, 4, 7, 11, 12, 29, 35, 18],\n",
    "    \"Square table Avg\": [None, 2, 6, 10, 9, 12, 15, 7],\n",
    "    \"Square table Max\": [None, 2, 8, 11, 17, 19, 17, 10],\n",
    "}\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index(\"Method\")\n",
    "\n",
    "# Melt the DataFrame to have a suitable form for seaborn's barplot\n",
    "df_melted = df.reset_index().melt(\n",
    "    id_vars=\"Method\", var_name=\"Task\", value_name=\"Success Rate\"\n",
    ")\n",
    "\n",
    "# Add 'Type' column to differentiate between Avg and Max\n",
    "df_melted[\"Metric\"] = df_melted[\"Task\"].str.extract(r\"(\\bAvg|\\bMax)\")\n",
    "df_melted[\"Task\"] = df_melted[\"Task\"].str.replace(r\" Avg\", \"\").str.replace(r\" Max\", \"\")\n",
    "\n",
    "# Plot the grouped barplot using catplot\n",
    "g = sns.catplot(\n",
    "    x=\"Task\",\n",
    "    y=\"Success Rate\",\n",
    "    hue=\"Method\",\n",
    "    # row=\"Metric\",\n",
    "    col=\"Metric\",\n",
    "    sharey=False,\n",
    "    data=df_melted,\n",
    "    kind=\"bar\",\n",
    "    # height=4,\n",
    "    # aspect=2,\n",
    "    height=5.0,  # 5.0 good\n",
    "    aspect=1.3,  # 1.0 good-ish\n",
    "    palette=\"husl\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Set titles and labels\n",
    "g.fig.subplots_adjust(top=0.9)  # adjust the Figure in rp\n",
    "# g.fig.suptitle('Success Rates (%) of Methods Across Tasks') # - Average vs Maximum')\n",
    "# g.set_titles(\"{col_name} {col_var}\")\n",
    "\n",
    "# Iterate over each subplot in the FacetGrid to set y-axis labels\n",
    "for ax, metric in zip(g.axes.flat, [\"Avg\", \"Max\"]):\n",
    "    ax.set_ylim([0, 82])\n",
    "    ax.set_ylabel(f\"{metric} Success Rate (%)\")\n",
    "    # ax.set_xlabel(f'Task')\n",
    "    ax.set_xlabel(f\"\")\n",
    "    ax.set_title(f\"\")\n",
    "\n",
    "    # Get current labels\n",
    "    labels = [label.get_text() for label in ax.get_xticklabels()]\n",
    "    # Replace spaces with newline characters as needed\n",
    "    new_labels = [label.replace(\" \", \"\\n\") for label in labels]\n",
    "    # Set new labels\n",
    "    ax.set_xticklabels(\n",
    "        new_labels\n",
    "    )  # , rotation=45)  # Optional: adjust rotation for better readability\n",
    "\n",
    "\n",
    "# Create the legend manually (TODO)\n",
    "# Manually creating the legend\n",
    "# Get the unique methods from your DataFrame to create legend entries\n",
    "methods = df_melted[\"Method\"].unique()\n",
    "\n",
    "# Use seaborn's color palette for consistency with the plot\n",
    "palette = sns.color_palette(\"husl\", len(methods))\n",
    "\n",
    "# Create legend handles\n",
    "legend_handles = [\n",
    "    plt.Line2D([0], [0], color=palette[i], marker=\"o\", linestyle=\"\", label=method)\n",
    "    for i, method in enumerate(methods)\n",
    "]\n",
    "\n",
    "# Add the legend to the figure\n",
    "# g.fig.legend(handles=legend_handles, title='Method', bbox_to_anchor=(0.75, 0.95), loc='upper left')\n",
    "# g.fig.legend(handles=legend_handles, title='Method', bbox_to_anchor=(0.875, 1.05), loc='upper left')\n",
    "g.figure.legend(\n",
    "    handles=legend_handles,\n",
    "    title=\"Method\",\n",
    "    bbox_to_anchor=(0.85, 0.975),\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Since we're adding a legend outside the plot, adjust the subplots to fit the figure area\n",
    "# g.fig.subplots_adjust(right=0.7)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(\"success_rate_barplot.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
